{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FbteuSO3tV5",
        "outputId": "9d0fad36-250c-4158-cf2c-8fb1e09a53e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langgraph in /home/fahad/.local/lib/python3.10/site-packages (0.3.2)\n",
            "Requirement already satisfied: langsmith in /home/fahad/.local/lib/python3.10/site-packages (0.3.11)\n",
            "Requirement already satisfied: langchain in /home/fahad/.local/lib/python3.10/site-packages (0.3.19)\n",
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.2.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /home/fahad/.local/lib/python3.10/site-packages (from langgraph) (0.3.40)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /home/fahad/.local/lib/python3.10/site-packages (from langgraph) (2.0.16)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /home/fahad/.local/lib/python3.10/site-packages (from langgraph) (0.1.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /home/fahad/.local/lib/python3.10/site-packages (from langgraph) (0.1.53)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (3.10.15)\n",
            "Requirement already satisfied: packaging>=23.2 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (24.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/fahad/.local/lib/python3.10/site-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /home/fahad/.local/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/fahad/.local/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain) (5.4.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/fahad/.local/lib/python3.10/site-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/fahad/.local/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /home/fahad/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/fahad/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /home/fahad/.local/lib/python3.10/site-packages (from langchain_groq) (0.18.0)\n",
            "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
            "  Downloading langchain_core-0.3.41-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/fahad/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/fahad/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /home/fahad/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/fahad/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: certifi in /home/fahad/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/fahad/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.5)\n",
            "Requirement already satisfied: idna in /home/fahad/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith) (2.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/fahad/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/fahad/.local/lib/python3.10/site-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /home/fahad/.local/lib/python3.10/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/fahad/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/fahad/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langsmith) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /home/fahad/.local/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fahad/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fahad/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith) (2.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/fahad/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/fahad/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/fahad/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
            "Downloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m537.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:02\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m674.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.41-py3-none-any.whl (415 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain_groq, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.40\n",
            "    Uninstalling langchain-core-0.3.40:\n",
            "      Successfully uninstalled langchain-core-0.3.40\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.19\n",
            "    Uninstalling langchain-0.3.19:\n",
            "      Successfully uninstalled langchain-0.3.19\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.20 langchain-core-0.3.41 langchain_community-0.3.19 langchain_groq-0.2.4 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 typing-inspect-0.9.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langsmith langchain langchain_groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7nby_Ijb4iDe"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em3GQzI_6K8H",
        "outputId": "ee1ee594-1699-4ca3-dcbd-5632cb873741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /home/fahad/.local/lib/python3.10/site-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from wikipedia) (4.10.0)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/fahad/.local/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/fahad/.local/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fahad/.local/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/fahad/.local/lib/python3.10/site-packages (from requests~=2.32.0->arxiv) (2023.7.22)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "Building wheels for collected packages: wikipedia, sgmllib3k\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=81e10b5d99965d9b471df7a00e5d946939797cb948b8efd5900d012efe313fc6\n",
            "  Stored in directory: /home/fahad/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=7699607ef721625a0686dfd5ab66f50ecd189ba5227ef0e3e357dad531e80031\n",
            "  Stored in directory: /home/fahad/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built wikipedia sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, wikipedia, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0 wikipedia-1.4.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zXI3qaom5a03"
      },
      "outputs": [],
      "source": [
        "## Working With Tools\n",
        "\n",
        "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper\n",
        "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
        "\n",
        "## Arxiv And Wikipedia tools\n",
        "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1,doc_content_chars_max=300)\n",
        "arxiv_tool=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
        "\n",
        "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=300)\n",
        "wiki_tool=WikipediaQueryRun(api_wrapper=api_wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "BdE09N5-5xCz",
        "outputId": "23a3c122-7bea-4668-a7bd-6f630df64dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Page: Shah Rukh Khan\\nSummary: Shah Rukh Khan (pronounced [ˈʃaːɦɾʊx xäːn] ; born 2 November 1965), also known by the initialism SRK, is an Indian actor and film producer who works in Hindi cinema. Referred to in the media as the \"Baadshah of Bollywood\" and \"King Khan\", he has appeared in more than 10'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wiki_tool.invoke(\"who is Sharukh Khan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "thYaNDYy5w_w",
        "outputId": "24d5bb06-6d9a-4dd0-c5db-ca4ae3e464b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Published: 2024-07-22\\nTitle: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\\nAuthors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\\nSummary: The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with \""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arxiv_tool.invoke(\"Attention is all you need\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ek52WFGI6lTq"
      },
      "outputs": [],
      "source": [
        "tools=[wiki_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eC6q3tTg6slO"
      },
      "outputs": [],
      "source": [
        "## Langgraph Application\n",
        "from langgraph.graph.message import add_messages\n",
        "class State(TypedDict):\n",
        "  messages:Annotated[list,add_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iXiurpCm7AWz"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph,START,END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X9nAxTc17JNh"
      },
      "outputs": [],
      "source": [
        "graph_builder= StateGraph(State)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Kc-4UeZa7SU_"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")  # Ensure this is a string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63tR5KIk7Zyd",
        "outputId": "a027ab06-88d5-4f7d-f119-8eefdd79a37e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c967c86af20>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c967c874340>, model_name='deepseek-r1-distill-llama-70b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = ChatGroq(api_key=groq_api_key, model_name=\"deepseek-r1-distill-llama-70b\")\n",
        "\n",
        "# print(llm)\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HS2TyqvY8BcL"
      },
      "outputs": [],
      "source": [
        "llm_with_tools=llm.bind_tools(tools=tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uhh-mYKn7sPU"
      },
      "outputs": [],
      "source": [
        "def chatbot(state:State):\n",
        "  return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "81qlR2Kl8uZL"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode,tools_condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3prnJx4M8YLF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7c967d136380>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_builder.add_node(\"chatbot\",chatbot)\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START,\"chatbot\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "q1ztKyS79-YE"
      },
      "outputs": [],
      "source": [
        "graph=graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "bg6et4vC-B_Y",
        "outputId": "0a887337-8b74-4cb9-adfd-3ab7abc47900"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1M_0ihg-OK3",
        "outputId": "1cd34556-05a4-4bec-d20e-945fe3c46a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there!, My name is John\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  wikipedia (call_a39k)\n",
            " Call ID: call_a39k\n",
            "  Args:\n",
            "    query: John\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fahad/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /home/fahad/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: wikipedia\n",
            "\n",
            "No good Wikipedia Search Result was found\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "<tool_call>{\"name\":\"wikipedia\",\"arguments\":{\"query\":\"John\"}}\n"
          ]
        }
      ],
      "source": [
        "user_input=\"Hi there!, My name is John\"\n",
        "\n",
        "events=graph.stream(\n",
        "     {\"messages\": [(\"user\", user_input)]},stream_mode=\"values\"\n",
        ")\n",
        "\n",
        "for event in events:\n",
        "  event[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnWrp7Xe-xey",
        "outputId": "abcb7807-11bd-4a09-dafc-bb32dfff3c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is RLHF.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  wikipedia (call_2wde)\n",
            " Call ID: call_2wde\n",
            "  Args:\n",
            "    query: RLHF\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: wikipedia\n",
            "\n",
            "Page: Reinforcement learning from human feedback\n",
            "Summary: In machine learning, reinforcement learning from human feedback (RLHF) is a technique to align an intelligent agent with human preferences. It involves training a reward model to represent preferences, which can then be used to train other mo\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Reinforcement Learning from Human Feedback (RLHF) is a machine learning technique used to align an AI system with human preferences and values. It involves training a model based on feedback provided by humans, allowing the AI to learn and improve its behavior over time.\n",
            "\n",
            "In RLHF, humans provide feedback in the form of rewards or penalties to guide the learning process. This feedback is used to train a reward model, which then generates rewards for the AI to optimize its actions. The process is particularly useful for tasks where it's difficult to specify a clear objective function but where human judgment can provide guidance.\n",
            "\n",
            "This approach is widely used in areas like natural language processing, robotics, and game playing, where the goal is to create AI systems that behave in ways that are aligned with human intentions and values.\n"
          ]
        }
      ],
      "source": [
        "user_input = \"what is RLHF.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [(\"user\", user_input)]},stream_mode=\"values\"\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
